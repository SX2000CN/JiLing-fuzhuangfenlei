"""
PyTorchè®­ç»ƒå™¨æ¨¡å—
è´Ÿè´£æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒåŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import numpy as np
from pathlib import Path
import json
import time
import logging
from typing import Dict, List, Tuple, Optional, Callable
from tqdm import tqdm
import matplotlib.pyplot as plt

from .model_factory import ModelFactory

logger = logging.getLogger(__name__)


class ClothingDataset(Dataset):
    """æœè£…å›¾ç‰‡æ•°æ®é›†"""
    
    def __init__(self, data_dir: str, transform=None, classes=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            transform: å›¾åƒå˜æ¢
            classes: ç±»åˆ«åˆ—è¡¨
        """
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.classes = classes or ['ä¸»å›¾', 'ç»†èŠ‚', 'åŠç‰Œ']
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        self.samples = []
        self._load_samples()
        
        logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆ: {len(self.samples)} ä¸ªæ ·æœ¬")
        for cls, count in self._count_samples().items():
            logger.info(f"  {cls}: {count} å¼ ")
    
    def _load_samples(self):
        """åŠ è½½æ ·æœ¬æ•°æ®"""
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        
        for class_name in self.classes:
            class_dir = self.data_dir / class_name
            if not class_dir.exists():
                logger.warning(f"ç±»åˆ«ç›®å½•ä¸å­˜åœ¨: {class_dir}")
                continue
            
            class_idx = self.class_to_idx[class_name]
            for img_path in class_dir.iterdir():
                if img_path.suffix.lower() in image_extensions:
                    self.samples.append((str(img_path), class_idx))
    
    def _count_samples(self) -> Dict[str, int]:
        """ç»Ÿè®¡å„ç±»åˆ«æ ·æœ¬æ•°é‡"""
        counts = {cls: 0 for cls in self.classes}
        for _, class_idx in self.samples:
            class_name = self.classes[class_idx]
            counts[class_name] += 1
        return counts
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, class_idx = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)
        
        return image, class_idx


class ClothingTrainer:
    """æœè£…åˆ†ç±»æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, 
                 model_name: str = 'efficientnetv2_s',
                 num_classes: int = 3,
                 device: str = 'auto',
                 input_size: int = 580):  # ä¸åˆ†ç±»å™¨ä¿æŒä¸€è‡´çš„580x580ç”œèœœç‚¹
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            num_classes: åˆ†ç±»æ•°é‡
            device: è®¡ç®—è®¾å¤‡
            input_size: è¾“å…¥å›¾åƒå°ºå¯¸ (ä¸åˆ†ç±»å™¨ä¸€è‡´ä½¿ç”¨580)
        """
        self.model_name = model_name
        self.num_classes = num_classes
        self.device = self._setup_device(device)
        self.input_size = input_size
        
        # è®­ç»ƒçŠ¶æ€
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        
        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'learning_rate': []
        }
        
        # å›è°ƒå‡½æ•°
        self.callbacks = []
        
        logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"  æ¨¡å‹: {model_name}")
        logger.info(f"  è®¾å¤‡: {self.device}")
        logger.info(f"  ç±»åˆ«æ•°: {num_classes}")
        logger.info(f"  è¾“å…¥å°ºå¯¸: {self.input_size}x{self.input_size} (ä¸åˆ†ç±»å™¨ä¸€è‡´çš„ç”œèœœç‚¹)")
        
        # æ¸…ç†GPUå†…å­˜
        self._cleanup_gpu_memory()
    
    def _cleanup_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            logger.info("GPUå†…å­˜å·²æ¸…ç†")
    
    def _get_gpu_memory_info(self):
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            cached = torch.cuda.memory_reserved() / 1024**3      # GB
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            return f"GPUå†…å­˜: {allocated:.1f}GBå·²åˆ†é…, {cached:.1f}GBå·²ç¼“å­˜, {total:.1f}GBæ€»è®¡"
        return "GPUä¸å¯ç”¨"
    
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == 'auto':
            if torch.cuda.is_available():
                device_obj = torch.device('cuda')
                logger.info(f"è‡ªåŠ¨é€‰æ‹©GPU: {torch.cuda.get_device_name(0)}")
            else:
                device_obj = torch.device('cpu')
                logger.info("GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        else:
            device_obj = torch.device(device)
            logger.info(f"ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}")
        
        return device_obj
    
    def add_callback(self, callback: Callable):
        """æ·»åŠ è®­ç»ƒå›è°ƒå‡½æ•°"""
        self.callbacks.append(callback)
    
    def _call_callbacks(self, event: str, **kwargs):
        """è°ƒç”¨å›è°ƒå‡½æ•°"""
        for callback in self.callbacks:
            if hasattr(callback, event):
                getattr(callback, event)(**kwargs)
    
    def prepare_data(self, train_dir: str, val_dir: str = None, 
                    batch_size: int = 32, num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            train_dir: è®­ç»ƒæ•°æ®ç›®å½•
            val_dir: éªŒè¯æ•°æ®ç›®å½•
            batch_size: æ‰¹æ¬¡å¤§å°
            num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•°
            
        Returns:
            è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        """
        # æ•°æ®å¢å¼º
        train_transform = transforms.Compose([
            transforms.Resize((self.input_size, self.input_size)),
            transforms.RandomHorizontalFlip(0.5),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        val_transform = transforms.Compose([
            transforms.Resize((self.input_size, self.input_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = ClothingDataset(train_dir, transform=train_transform)
        
        if val_dir:
            val_dataset = ClothingDataset(val_dir, transform=val_transform)
        else:
            # ä»è®­ç»ƒé›†ä¸­åˆ†å‰²éªŒè¯é›†
            total_size = len(train_dataset)
            val_size = int(0.2 * total_size)
            train_size = total_size - val_size
            
            train_dataset, val_dataset = torch.utils.data.random_split(
                train_dataset, [train_size, val_size]
            )
            # ä¸ºéªŒè¯é›†è®¾ç½®å˜æ¢
            val_dataset.dataset.transform = val_transform
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆè®­ç»ƒæ—¶å‡å°‘num_workersé¿å…å†…å­˜é—®é¢˜ï¼‰
        safe_num_workers = min(num_workers, 2)  # é™åˆ¶workeræ•°é‡
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True, 
            num_workers=safe_num_workers,
            pin_memory=True if self.device.type == 'cuda' else False,
            persistent_workers=False  # é¿å…æŒä¹…workerå ç”¨å†…å­˜
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=safe_num_workers,
            pin_memory=True if self.device.type == 'cuda' else False,
            persistent_workers=False
        )
        
        logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆ:")
        logger.info(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        logger.info(f"  éªŒè¯æ ·æœ¬: {len(val_dataset)}")
        logger.info(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        return train_loader, val_loader
    
    def build_model(self, pretrained: bool = True):
        """æ„å»ºæ¨¡å‹"""
        self.model = ModelFactory.create_model(
            self.model_name, 
            num_classes=self.num_classes, 
            pretrained=pretrained
        )
        self.model.to(self.device)
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        logger.info("æ¨¡å‹æ„å»ºå®Œæˆ")
        return self.model
    
    def setup_optimizer(self, lr: float = 0.001, weight_decay: float = 1e-4):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=50, eta_min=1e-6
        )
        
        logger.info(f"ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ: lr={lr}, weight_decay={weight_decay}")
    
    def create_data_loaders(self, data_dir: str, batch_size: int = 32, 
                           val_split: float = 0.2, num_workers: int = 0) -> Tuple[DataLoader, DataLoader]:
        """
        åˆ›å»ºæ•°æ®åŠ è½½å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            batch_size: æ‰¹æ¬¡å¤§å°
            val_split: éªŒè¯é›†æ¯”ä¾‹
            num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•°
            
        Returns:
            è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        """
        # æ•°æ®å¢å¼º
        train_transform = transforms.Compose([
            transforms.Resize((self.input_size, self.input_size)),
            transforms.RandomHorizontalFlip(0.5),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        val_transform = transforms.Compose([
            transforms.Resize((self.input_size, self.input_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # åˆ›å»ºå®Œæ•´æ•°æ®é›†
        full_dataset = ClothingDataset(data_dir, transform=train_transform)
        
        # åˆ†å‰²æ•°æ®é›†
        total_size = len(full_dataset)
        val_size = int(val_split * total_size)
        train_size = total_size - val_size
        
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size]
        )
        
        # ä¸ºéªŒè¯é›†å•ç‹¬è®¾ç½®å˜æ¢
        # åˆ›å»ºéªŒè¯é›†çš„å‰¯æœ¬å¹¶è®¾ç½®ä¸åŒçš„å˜æ¢
        val_dataset_copy = ClothingDataset(data_dir, transform=val_transform)
        
        # è·å–éªŒè¯é›†çš„ç´¢å¼•
        val_indices = val_dataset.indices
        val_samples = [val_dataset_copy.samples[i] for i in val_indices]
        val_dataset_copy.samples = val_samples
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆè®­ç»ƒæ—¶å‡å°‘num_workersé¿å…å†…å­˜é—®é¢˜ï¼‰
        safe_num_workers = min(num_workers, 2)  # é™åˆ¶workeræ•°é‡
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True, 
            num_workers=safe_num_workers,
            pin_memory=True if self.device.type == 'cuda' else False,
            persistent_workers=False  # é¿å…æŒä¹…workerå ç”¨å†…å­˜
        )
        
        val_loader = DataLoader(
            val_dataset_copy, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=safe_num_workers,
            pin_memory=True if self.device.type == 'cuda' else False,
            persistent_workers=False
        )
        
        logger.info(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
        logger.info(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        logger.info(f"  éªŒè¯æ ·æœ¬: {len(val_dataset)}")
        logger.info(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        return train_loader, val_loader
    
    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        # æ˜¾ç¤ºè®­ç»ƒå¼€å§‹æ—¶çš„GPUå†…å­˜çŠ¶æ€
        logger.info(f"è®­ç»ƒå¼€å§‹: {self._get_gpu_memory_info()}")
        
        with tqdm(train_loader, desc="è®­ç»ƒä¸­") as pbar:
            for batch_idx, (images, labels) in enumerate(pbar):
                images, labels = images.to(self.device), labels.to(self.device)
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                
                # ç»Ÿè®¡
                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                # å®šæœŸæ¸…ç†GPUå†…å­˜
                if batch_idx % 50 == 0 and batch_idx > 0:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100.*correct/total:.2f}%'
                })
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc
    
    def validate_epoch(self, val_loader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            with tqdm(val_loader, desc="éªŒè¯ä¸­") as pbar:
                for images, labels in pbar:
                    images, labels = images.to(self.device), labels.to(self.device)
                    
                    outputs = self.model(images)
                    loss = self.criterion(outputs, labels)
                    
                    running_loss += loss.item()
                    _, predicted = outputs.max(1)
                    total += labels.size(0)
                    correct += predicted.eq(labels).sum().item()
                    
                    pbar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Acc': f'{100.*correct/total:.2f}%'
                    })
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc
    
    def train(self, 
              train_loader: DataLoader, 
              val_loader: DataLoader,
              epochs: int = 50,
              save_dir: str = 'models/checkpoints',
              save_best: bool = True) -> Dict:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            save_best: æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
            
        Returns:
            è®­ç»ƒå†å²
        """
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        best_acc = 0.0
        start_time = time.time()
        
        self._call_callbacks('on_train_begin', epochs=epochs)
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            self._call_callbacks('on_epoch_begin', epoch=epoch)
            
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self.validate_epoch(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler:
                self.scheduler.step()
                current_lr = self.scheduler.get_last_lr()[0]
            else:
                current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            self.history['learning_rate'].append(current_lr)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if save_best and val_acc > best_acc:
                best_acc = val_acc
                self.save_model(save_path / 'best_model.pth', epoch, val_acc)
                logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: éªŒè¯å‡†ç¡®ç‡ {val_acc:.2f}%")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                self.save_model(save_path / f'checkpoint_epoch_{epoch+1}.pth', epoch, val_acc)
            
            epoch_time = time.time() - epoch_start
            
            # æ‰“å°è¿›åº¦
            logger.info(f"Epoch [{epoch+1}/{epochs}] - "
                       f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - "
                       f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% - "
                       f"LR: {current_lr:.6f} - Time: {epoch_time:.2f}s")
            
            self._call_callbacks('on_epoch_end', 
                               epoch=epoch, 
                               train_loss=train_loss,
                               train_acc=train_acc,
                               val_loss=val_loss,
                               val_acc=val_acc,
                               lr=current_lr)
        
        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/60:.2f}åˆ†é’Ÿ, æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
        
        self._call_callbacks('on_train_end', best_acc=best_acc, total_time=total_time)
        
        return self.history
    
    def save_model(self, path: str, epoch: int, accuracy: float):
        """ä¿å­˜æ¨¡å‹"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer else None,
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'model_name': self.model_name,
            'num_classes': self.num_classes,
            'input_size': self.input_size,
            'epoch': epoch,
            'accuracy': accuracy,
            'history': self.history,
            'timestamp': time.time()
        }
        
        torch.save(checkpoint, path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        try:
            checkpoint = torch.load(path, map_location=self.device)
            
            # é‡å»ºæ¨¡å‹
            if not self.model:
                self.build_model(pretrained=False)
            
            # åŠ è½½æ¨¡å‹æƒé‡
            self.model.load_state_dict(checkpoint['model_state_dict'])
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            if self.optimizer and 'optimizer_state_dict' in checkpoint:
                try:
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    logger.info("ä¼˜åŒ–å™¨çŠ¶æ€å·²æ¢å¤")
                except Exception as e:
                    logger.warning(f"ä¼˜åŒ–å™¨çŠ¶æ€æ¢å¤å¤±è´¥: {e}")
            
            # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            if self.scheduler and 'scheduler_state_dict' in checkpoint:
                try:
                    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    logger.info("å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å·²æ¢å¤")
                except Exception as e:
                    logger.warning(f"å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€æ¢å¤å¤±è´¥: {e}")
            
            # åŠ è½½è®­ç»ƒå†å²ï¼ˆå¯é€‰ï¼‰
            if 'history' in checkpoint:
                self.history = checkpoint['history']
                logger.info("è®­ç»ƒå†å²å·²æ¢å¤")
            
            logger.info(f"æ¨¡å‹å·²åŠ è½½: {path}")
            logger.info(f"  Epoch: {checkpoint.get('epoch', 'unknown')}")
            accuracy = checkpoint.get('accuracy', 'unknown')
            if accuracy != 'unknown':
                logger.info(f"  å‡†ç¡®ç‡: {accuracy:.2f}%")
            else:
                logger.info(f"  å‡†ç¡®ç‡: {accuracy}")
            
            return checkpoint
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def plot_history(self, save_path: str = None):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        if not self.history['train_loss']:
            logger.warning("æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(self.history['train_loss'], label='Train Loss')
        axes[0, 0].plot(self.history['val_loss'], label='Val Loss')
        axes[0, 0].set_title('Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid()
        
        # å‡†ç¡®ç‡æ›²çº¿
        axes[0, 1].plot(self.history['train_acc'], label='Train Acc')
        axes[0, 1].plot(self.history['val_acc'], label='Val Acc')
        axes[0, 1].set_title('Accuracy')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy (%)')
        axes[0, 1].legend()
        axes[0, 1].grid()
        
        # å­¦ä¹ ç‡æ›²çº¿
        axes[1, 0].plot(self.history['learning_rate'])
        axes[1, 0].set_title('Learning Rate')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].grid()
        
        # éªŒè¯å‡†ç¡®ç‡è¯¦ç»†
        axes[1, 1].plot(self.history['val_acc'], 'g-', linewidth=2)
        axes[1, 1].set_title('Validation Accuracy Detail')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Accuracy (%)')
        axes[1, 1].grid()
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜: {save_path}")
        
        return fig


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
    
    print("ğŸ§ª æµ‹è¯•è®­ç»ƒå™¨...")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ClothingTrainer(
        model_name='efficientnetv2_s',
        num_classes=3,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # æ„å»ºæ¨¡å‹
    model = trainer.build_model()
    print(f"âœ… æ¨¡å‹æ„å»ºæˆåŠŸ: {type(model).__name__}")
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    trainer.setup_optimizer(lr=0.001)
    print("âœ… ä¼˜åŒ–å™¨è®¾ç½®æˆåŠŸ")
    
    print("âœ… è®­ç»ƒå™¨æµ‹è¯•å®Œæˆ")
